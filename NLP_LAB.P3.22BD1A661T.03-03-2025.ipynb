{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe34664-d6d8-4e05-82a6-fe4d450c9571",
   "metadata": {},
   "source": [
    "## program 3\n",
    " Download Wikipedia's page on open source and convert the text to its native forms. Try it with various stemming and lemmatization modules. Use Python's timer module to measure their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5609633b-5aa5-44f8-8ebf-56755b275451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import time\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer,SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7715796e-fc6b-45a2-a654-ce0029480745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tangu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tangu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tangu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cee821b-f75e-4d94-bca3-cd559ce1b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Wikipedia page\n",
    "def get_wikipedia_page(title):\n",
    "    user_agent = \"MyWikipediaBot/1.0\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(user_agent=user_agent, language='en')\n",
    "    page = wiki_wiki.page(title)\n",
    "    return page.text if page.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2afaa05d-1d6f-417a-bff4-f9a768260503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c01ced1f-3ff8-4da8-bb32-69baf3bc2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming and lemmatization with performance measurement\n",
    "def process_text(tokens):\n",
    "    ps = PorterStemmer()\n",
    "    ls = LancasterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    ss = SnowballStemmer(\"english\")  # Initialize Snowball Stemmer\n",
    "\n",
    "    # Measure time for SnowballStemmer\n",
    "    start = time.time()\n",
    "    snowball_stems = [ss.stem(word) for word in tokens]\n",
    "    snowball_time = time.time() - start\n",
    "\n",
    "    \n",
    "    # Measure time for PorterStemmer\n",
    "    start = time.time()\n",
    "    porter_stems = [ps.stem(word) for word in tokens]\n",
    "    porter_time = time.time() - start\n",
    "\n",
    "    # Measure time for LancasterStemmer\n",
    "    start = time.time()\n",
    "    lancaster_stems = [ls.stem(word) for word in tokens]\n",
    "    lancaster_time = time.time() - start\n",
    "\n",
    "    # Measure time for WordNet Lemmatizer\n",
    "    start = time.time()\n",
    "    wordnet_lems = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    wordnet_time = time.time() - start\n",
    "\n",
    "    # Measure time for Spacy Lemmatizer\n",
    "    start = time.time()\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    spacy_lems = [token.lemma_ for token in doc]\n",
    "    spacy_time = time.time() - start\n",
    "\n",
    "\n",
    "     # Print reduced tokens\n",
    "    print(\"\\nReduced Tokens:\")\n",
    "    print(\"\\nPorterStemmer:\", porter_stems[:50])  # Print first 50 words for readability\n",
    "    print(\"\\nLancasterStemmer:\", lancaster_stems[:50])\n",
    "    print(\"\\nSnowballStemmer:\", snowball_stems[:50])\n",
    "    print(\"\\nWordNet Lemmatizer:\", wordnet_lems[:50])\n",
    "    print(tokens[:50])\n",
    "    print(\"\\nSpacy Lemmatizer:\", spacy_lems[:50])\n",
    "\n",
    "    print('\\n\\n\\n\\n')\n",
    "\n",
    "    # Print results\n",
    "    print(f\"PorterStemmer Time: {porter_time:.4f} sec\")\n",
    "    print(f\"LancasterStemmer Time: {lancaster_time:.4f} sec\")\n",
    "    print(f\"SnowballStemmer Time: {snowball_time:.4f} sec\")\n",
    "    print(f\"WordNetLemmatizer Time: {wordnet_time:.4f} sec\")\n",
    "    print(f\"Spacy Lemmatizer Time: {spacy_time:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d730382c-5abe-429d-9421-704ca5dc99f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduced Tokens:\n",
      "\n",
      "PorterStemmer: ['mahendra', 'singh', 'dhoni', '(', ';', 'born', '7', 'juli', '1981', ')', 'is', 'an', 'indian', 'profession', 'cricket', 'who', 'play', 'as', 'a', 'right-hand', 'batter', 'and', 'a', 'wicket-keep', '.', 'wide', 'regard', 'as', 'one', 'of', 'the', 'most', 'prolif', 'wicket-keep', 'batsmen', 'and', 'captain', ',', 'he', 'repres', 'the', 'indian', 'cricket', 'team', 'and', 'wa', 'the', 'captain', 'of', 'the']\n",
      "\n",
      "LancasterStemmer: ['mahendr', 'singh', 'dhon', '(', ';', 'born', '7', 'july', '1981', ')', 'is', 'an', 'ind', 'profess', 'cricket', 'who', 'play', 'as', 'a', 'right-handed', 'bat', 'and', 'a', 'wicket-keeper', '.', 'wid', 'regard', 'as', 'on', 'of', 'the', 'most', 'prol', 'wicket-keeper', 'batsm', 'and', 'captain', ',', 'he', 'repres', 'the', 'ind', 'cricket', 'team', 'and', 'was', 'the', 'captain', 'of', 'the']\n",
      "\n",
      "SnowballStemmer: ['mahendra', 'singh', 'dhoni', '(', ';', 'born', '7', 'juli', '1981', ')', 'is', 'an', 'indian', 'profession', 'cricket', 'who', 'play', 'as', 'a', 'right-hand', 'batter', 'and', 'a', 'wicket-keep', '.', 'wide', 'regard', 'as', 'one', 'of', 'the', 'most', 'prolif', 'wicket-keep', 'batsmen', 'and', 'captain', ',', 'he', 'repres', 'the', 'indian', 'cricket', 'team', 'and', 'was', 'the', 'captain', 'of', 'the']\n",
      "\n",
      "WordNet Lemmatizer: ['Mahendra', 'Singh', 'Dhoni', '(', ';', 'born', '7', 'July', '1981', ')', 'is', 'an', 'Indian', 'professional', 'cricketer', 'who', 'play', 'a', 'a', 'right-handed', 'batter', 'and', 'a', 'wicket-keeper', '.', 'Widely', 'regarded', 'a', 'one', 'of', 'the', 'most', 'prolific', 'wicket-keeper', 'batsman', 'and', 'captain', ',', 'he', 'represented', 'the', 'Indian', 'cricket', 'team', 'and', 'wa', 'the', 'captain', 'of', 'the']\n",
      "['Mahendra', 'Singh', 'Dhoni', '(', ';', 'born', '7', 'July', '1981', ')', 'is', 'an', 'Indian', 'professional', 'cricketer', 'who', 'plays', 'as', 'a', 'right-handed', 'batter', 'and', 'a', 'wicket-keeper', '.', 'Widely', 'regarded', 'as', 'one', 'of', 'the', 'most', 'prolific', 'wicket-keeper', 'batsmen', 'and', 'captains', ',', 'he', 'represented', 'the', 'Indian', 'cricket', 'team', 'and', 'was', 'the', 'captain', 'of', 'the']\n",
      "\n",
      "Spacy Lemmatizer: ['Mahendra', 'Singh', 'Dhoni', '(', ';', 'bear', '7', 'July', '1981', ')', 'be', 'an', 'indian', 'professional', 'cricketer', 'who', 'play', 'as', 'a', 'right', '-', 'handed', 'batter', 'and', 'a', 'wicket', '-', 'keeper', '.', 'widely', 'regard', 'as', 'one', 'of', 'the', 'most', 'prolific', 'wicket', '-', 'keeper', 'batsman', 'and', 'captain', ',', 'he', 'represent', 'the', 'indian', 'cricket', 'team']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PorterStemmer Time: 0.0554 sec\n",
      "LancasterStemmer Time: 0.0350 sec\n",
      "SnowballStemmer Time: 0.0641 sec\n",
      "WordNetLemmatizer Time: 0.0625 sec\n",
      "Spacy Lemmatizer Time: 1.0011 sec\n"
     ]
    }
   ],
   "source": [
    "text = get_wikipedia_page(\"MS_Dhoni\")\n",
    "tokens = tokenize(text)\n",
    "process_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d29e8-1472-4a1d-a561-4f25a7121c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d9ed3-4eb7-4c9d-83f2-b60e1c6b2798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
