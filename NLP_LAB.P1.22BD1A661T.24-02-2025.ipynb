{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d3673e-9f2d-45d5-873c-1e7168491f5d",
   "metadata": {},
   "source": [
    "# SpaCy Overview & Key Concepts\n",
    "\n",
    "## 1. Main Components of the SpaCy `Doc` Object\n",
    "\n",
    "A `Doc` is the container for the entire processed text. Its main components include:\n",
    "\n",
    "- **Tokens:** Individual words, punctuation, or symbols. Each token has attributes like `.text`, `.lemma_`, `.pos_`, and more.\n",
    "- **Sentences:** The `Doc` object can be segmented into sentences.\n",
    "- **Linguistic Annotations:** Includes part-of-speech tags, dependency parse information, named entities, etc.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How SpaCy Differs from NLTK\n",
    "\n",
    "- **Performance & Production Use:**  \n",
    "  SpaCy is built for speed and production-level usage with efficient, pre-trained models. NLTK, in contrast, is primarily designed for teaching, research, and prototyping.\n",
    "\n",
    "- **Ease of Use & Consistency:**  \n",
    "  SpaCy provides a consistent API for advanced NLP tasks (POS tagging, NER, dependency parsing) using state-of-the-art models, while NLTK offers a wide range of algorithms and corpora but with less consistency in API design.\n",
    "\n",
    "- **Modern Machine Learning:**  \n",
    "  SpaCy leverages modern neural network architectures for its NLP pipelines, whereas NLTK traditionally relies on rule-based and statistical methods.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Customizing Rules in SpaCy\n",
    "\n",
    "SpaCy allows you to add custom rules using components like the **EntityRuler** and **Matcher**.\n",
    "\n",
    "**EntityRuler Example:**\n",
    "```python\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Add EntityRuler before the built-in NER\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# Define custom patterns\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Apple Inc. is a tech company.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "```\n",
    "This code snippet demonstrates how to create and integrate custom entity rules.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. How SpaCy Performs POS Tagging & Named Entity Recognition (NER)\n",
    "\n",
    "SpaCyâ€™s pipeline uses statistical models (often based on neural networks) that have been trained on large annotated datasets. When you call the `nlp` object on text, it automatically performs:\n",
    "\n",
    "- **POS Tagging:** Assigning part-of-speech labels to each token.\n",
    "- **NER:** Identifying and categorizing named entities in the text.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
    "\n",
    "# POS Tagging\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)\n",
    "\n",
    "# Named Entity Recognition\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. How SpaCy Measures Text Similarity\n",
    "\n",
    "SpaCy computes similarity by comparing the vector representations (word embeddings) of texts. You can compare:\n",
    "\n",
    "- **Doc-to-Doc:** Using `doc.similarity(other_doc)`\n",
    "- **Span-to-Span:** Using `span.similarity(other_span)`\n",
    "- **Token-to-Token:** Using `token.similarity(other_token)`\n",
    "\n",
    "*Note:* To get meaningful similarity scores, use a model with word vectors (e.g., `en_core_web_md` or `en_core_web_lg`).\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load a model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc1 = nlp(\"I like apples\")\n",
    "doc2 = nlp(\"I enjoy oranges\")\n",
    "\n",
    "print(\"Similarity score:\", doc1.similarity(doc2))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Saving and Loading Trained SpaCy Models\n",
    "\n",
    "After training or customizing a model, you can save it to disk and later load it.\n",
    "\n",
    "**Save the Model:**\n",
    "```python\n",
    "nlp.to_disk(\"my_spacy_model\")\n",
    "```\n",
    "\n",
    "**Load the Model:**\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"my_spacy_model\")\n",
    "```\n",
    "\n",
    "This makes it easy to persist and reuse your models without retraining.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Differentiating Between `Doc`, `Span`, and `Token` Objects\n",
    "\n",
    "- **Doc:**  \n",
    "  The `Doc` object represents the entire processed document. It is an ordered sequence of tokens and holds all linguistic annotations for the document.\n",
    "\n",
    "- **Span:**  \n",
    "  A `Span` is a slice of the `Doc`, representing a continuous subset of tokens (e.g., a phrase or sentence). Spans share the annotations of the parent `Doc`.\n",
    "\n",
    "- **Token:**  \n",
    "  Each `Token` represents an individual element (word, punctuation, etc.) from the text. Tokens are the smallest units in spaCy's processing pipeline and carry individual attributes like text, lemma, POS tag, etc.\n",
    "\n",
    "**Visualization Example:**\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"SpaCy makes NLP tasks fun and efficient.\")\n",
    "\n",
    "# Accessing Tokens\n",
    "print(\"Tokens:\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "# Creating a Span (first 3 tokens)\n",
    "span = doc[0:3]\n",
    "print(\"\\nSpan:\", span.text)\n",
    "\n",
    "# The entire Doc\n",
    "print(\"\\nDoc:\", doc.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b7d8a-d365-4643-8db7-1f20c0ebfa9e",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "Create a basic NLP program to find words, phrases, names and concepts using \"spacy.blank\" to \n",
    "create the English nlp object. Process the text and instantiate a Doc object in the variable doc. Select the first token of the Doc and print its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6c640d2-2433-48f0-81d2-7fddad1f5c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank English NLP object\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text to create a Doc object\n",
    "doc = nlp(\"Apple is a technology company.\")\n",
    "\n",
    "# Select the first token of the Doc\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the text of the first token\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de753093-077a-4da8-a29b-699bc6160f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words (Tokens):\n",
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "a\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n",
      ".\n",
      "This\n",
      "boosts\n",
      "the\n",
      "U.K.\n",
      "economy\n",
      "Sentences: ['Apple is looking at buying a U.K. startup for $1 billion.', 'This boosts the U.K. economy']\n",
      "\n",
      "Stop words\n",
      "original text: Apple is looking at buying a U.K. startup for $1 billion. This boosts the U.K. economy\n",
      "Filtered text: Apple looking buying U.K. startup $ 1 billion . boosts U.K. economy\n",
      "\n",
      "Noun Phrases:\n",
      "Apple\n",
      "a U.K.\n",
      "This\n",
      "the U.K. economy\n",
      "\n",
      "Named Entities:\n",
      "Apple - ORG\n",
      "U.K. - GPE\n",
      "$1 billion - MONEY\n",
      "U.K. - GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple is looking at buying a U.K. startup for $1 billion. This boosts the U.K. economy\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# extract words \n",
    "print(\"Words (Tokens):\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    \n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(\"Sentences:\", sentences)\n",
    "\n",
    "# stop words removal\n",
    "print('\\nStop words')\n",
    "stop_word=[token.text for token in doc if not token.is_stop]\n",
    "stop_words=' '.join(stop_word)\n",
    "print(f\"original text: {text}\")\n",
    "print(f\"Filtered text: {stop_words}\")\n",
    "\n",
    "print(\"\\nNoun Phrases:\")\n",
    "if \"parser\" in nlp.pipe_names: \n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(chunk.text)\n",
    "else:\n",
    "    print(\"No parser available in blank model. Noun chunking requires 'parser'.\")\n",
    "\n",
    "print(\"\\nNamed Entities:\")\n",
    "if \"ner\" in nlp.pipe_names:\n",
    "    for ent in doc.ents:\n",
    "        print(f\"{ent.text} - {ent.label_}\")\n",
    "else:\n",
    "    print(\"No named entity recognizer (NER) available in blank model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbdf386-836e-4441-9411-f7a5dea0b8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8818f5-520b-418e-bd37-b5a6a1b3e81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
